{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORING IMAGE CLASSIFICATION USING CNNs AND KERNEL METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ho diviso il file nei rispettivi chunk di codice. Il dataset usato è CIFAR-10. Ho applicato data augmentation sul training set per avere diversità nel training e ridurre overfitting.\n",
    "\n",
    "- **Parametri di regolarizzazione Res-Net**: dropout\n",
    "- **Parametri di regolarizzazione SVM**: C\n",
    "- **Parametri di regolarizzazione SVM con RFF**: C, $\\gamma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazioni per il training (con data augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Flip orizzontale casuale\n",
    "    transforms.RandomCrop(32, padding=4),  # Crop casuale con padding\n",
    "    transforms.ToTensor(),  # Converti in tensore\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione\n",
    "])\n",
    "\n",
    "# Trasformazioni per validazione e test (senza data augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset per training e test\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "\n",
    "# Applica trasformazioni diverse per training e validazione\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=True, transform=val_test_transform, download=True)\n",
    "\n",
    "# Applica trasformazione per il test\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=val_test_transform, download=True)\n",
    "\n",
    "# Divisione del training set in training (80%) e validazione (20%)\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% per il training\n",
    "val_size = len(full_train_dataset) - train_size  # 20% per la validazione\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader per batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Training con shuffle\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)  # Validazione senza shuffle\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Test senza shuffle\n",
    "\n",
    "# Stampiamo il numero di esempi\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-34 + Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload pre trained model\n",
    "model = models.resnet34(weights='DEFAULT')\n",
    "# Modifica dell'ultimo fully connected e applica  Dropout\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3), \n",
    "    nn.Linear(model.fc.in_features, 10)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss e ottimizzatore\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# Funzioni di addestramento e validazione\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "# Ciclo di addestramento\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Testare il modello\n",
    "test_loss, test_acc = validate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-34 + SVM end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di perdita personalizzata con Square Hinge Loss + L2 Regularization\n",
    "def square_hinge_loss(outputs, targets, weights, C=10):\n",
    "    \n",
    "    # Creazione del tensore one-hot per i target (classi vere)\n",
    "    targets_one_hot = torch.full_like(outputs, -1, device=outputs.device)  # Tutte le classi inizialmente a -1\n",
    "    targets_one_hot[torch.arange(len(targets)), targets] = 1  # La classe corretta a +1\n",
    "    \n",
    "    # Calcolo del margine\n",
    "    margins = 1 - targets_one_hot * outputs  # shape: (n_samples, n_classes)\n",
    "    # Square hinge loss: max(0, margin)^2\n",
    "    hinge_loss = torch.clamp(margins, min=0) ** 2  # shape: (n_samples, n_classes)\n",
    "    \n",
    "    # Media su tutti i campioni e classi\n",
    "    hinge_loss = hinge_loss.mean()\n",
    "\n",
    "    # Calcolo della regolarizzazione L2 (media dei quadrati dei pesi)\n",
    "    reg_loss = torch.mean(torch.square(weights))\n",
    "    # Loss complessiva con il parametro di penalizzazione C\n",
    "    total_loss = C*hinge_loss+reg_loss\n",
    "    \n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Caricamento del modello predefinito (ResNet34)\n",
    "model = models.resnet34(weights='DEFAULT')\n",
    "\n",
    "# Modifica dell'ultimo fully connected con Dropout\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),  \n",
    "    nn.Linear(model.fc.in_features, 10)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Ottimizzatore\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# Funzioni di addestramento e validazione\n",
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Output del modello (logit)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Pesi del layer finale\n",
    "        readout_weights = model.fc[1].weight\n",
    "\n",
    "        # Calcolare la square hinge loss con L2 regularization\n",
    "        loss = square_hinge_loss(outputs, targets, readout_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Funzione di validazione\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Pesi del layer finale\n",
    "            readout_weights = model.fc[1].weight\n",
    "\n",
    "            # Calcolare la square hinge loss con L2 regularization\n",
    "            loss = square_hinge_loss(outputs, targets, readout_weights)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "# Ciclo di addestramento\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Testare il modello\n",
    "test_loss, test_acc = validate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testare il modello\n",
    "test_loss, test_acc = validate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-34 + Kernel SVM end-to-end using Random Fourier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare Random Fourier Features (RFF)\n",
    "class RandomFourierFeatures(nn.Module):\n",
    "    def __init__(self, input_dim, sigma=1):\n",
    "        super(RandomFourierFeatures, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.scale = sigma\n",
    "        \n",
    "        # Pesi fissi per RFF: un vettore di 512 pesi\n",
    "        self.register_buffer(\"weights\", torch.normal(mean=0, std=torch.sqrt(torch.tensor(sigma)), size=(input_dim,)))  # Vettore di 512 pesi\n",
    "        self.register_buffer(\"bias\", 2 * np.pi * torch.rand(input_dim))  # Bias casuali per l'RFF\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Moltiplicazione element-wise dell'input x con il vettore di pesi\n",
    "        projections = x * self.weights  # Moltiplicazione element-wise dell'input con i pesi\n",
    "        \n",
    "        # Aggiunta del bias\n",
    "        projections = projections + self.bias  # Aggiunta del bias alla proiezione\n",
    "        \n",
    "        # Coseno della trasformazione\n",
    "        projections = torch.cos(projections)  # Applichiamo il coseno\n",
    "        \n",
    "        # Moltiplichiamo per sqrt(2 / D), dove D è la dimensione dell'input (input_dim)\n",
    "        return torch.sqrt(torch.tensor(2.0 / self.input_dim)) * projections\n",
    "\n",
    "# Square hinge loss come definita in precedenza\n",
    "def square_hinge_loss(outputs, targets, weights, C=1):\n",
    "    targets_one_hot = torch.full_like(outputs, -1, device=outputs.device)\n",
    "    targets_one_hot[torch.arange(len(targets)), targets] = 1\n",
    "\n",
    "    margins = 1 - targets_one_hot * outputs\n",
    "    hinge_loss = torch.clamp(margins, min=0) ** 2\n",
    "    hinge_loss = hinge_loss.mean()\n",
    "\n",
    "    reg_loss = torch.mean(torch.square(weights))\n",
    "    total_loss = C * hinge_loss + reg_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Caricamento della ResNet34 predefinita\n",
    "model = models.resnet34(weights='DEFAULT')\n",
    "\n",
    "# Extract the embedding dimension from ResNet (which is 512 for ResNet34)\n",
    "embedding_dim = model.fc.in_features \n",
    "\n",
    "# Define RFF layer, ensuring the output dimension stays the same as the input dimension\n",
    "rff_layer = RandomFourierFeatures(embedding_dim)\n",
    "\n",
    "# Replace the fully connected layer with a custom sequence (dropout, RFF, final linear layer)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    rff_layer,  # Apply Random Fourier Features\n",
    "    nn.Linear(embedding_dim, 10)  # Final classification layer\n",
    ")\n",
    "\n",
    "# Move the model to the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# Ottimizzatore e scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# Funzioni di addestramento e validazione\n",
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        readout_weights = model.fc[-1].weight\n",
    "        loss = square_hinge_loss(outputs, targets, readout_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            readout_weights = model.fc[-1].weight\n",
    "            loss = square_hinge_loss(outputs, targets, readout_weights)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ciclo di addestramento\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Testare il modello\n",
    "test_loss, test_acc = validate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
