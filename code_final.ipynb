{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORING IMAGE CLASSIFICATION USING CNNs AND KERNEL METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of ResNet from Scratch with Three Classification Layers\n",
    "\n",
    "In this notebook, we implement a ResNet18 and a ResNet34 architecture from scratch and develop three different classification layers:\n",
    "\n",
    "1. **Standard Classification Layer**: Using a softmax activation function.\n",
    "2. **SVM Classification Layer**: Incorporating support vector machine principles.\n",
    "3. **Kernel SVM Approximation**: Using Random Fourier Features (RFF) to approximate kernel SVM behavior.\n",
    "\n",
    "You can experiment with various hyperparameters to observe their effects on the model performance:\n",
    "\n",
    "- **ResNet+Softmax Regularization Parameters**: Dropout\n",
    "- **ResNet+SVM Regularization Parameters**: Dropout, C\n",
    "- **ResNet+SVM with RFF Regularization Parameters**: Dropout, C, $\\gamma$, $D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split, Subset, ConcatDataset\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)  # For CPU\n",
    "    torch.cuda.manual_seed_all(seed)  # For all GPUs\n",
    "    np.random.seed(seed)  # For NumPy\n",
    "    random.seed(seed)  # For Python random\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# Mean and standard deviation for normalization (CIFAR-10 specific)\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "# Transformations for the training set (includes data augmentation)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.6),   # Horizontal flip with probability 0.5\n",
    "    transforms.RandomCrop(32, padding=4),     # Random crop with padding of 4 pixels\n",
    "    transforms.RandomRotation(15),           # Random rotation of Â±15 degrees\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  ## Uncomment if needed ##\n",
    "    transforms.ToTensor(),                    # Convert image to tensor\n",
    "    transforms.Normalize(mean, std)          # Normalize with mean and std\n",
    "])\n",
    "# Transformations for the validation and test set (no data augmentation)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                    # Convert image to tensor\n",
    "    transforms.Normalize(mean, std)          # Normalize with mean and std\n",
    "])\n",
    "# Load CIFAR-10 training dataset both augmented and not augmented\n",
    "dataset_augmented = CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
    "dataset_normal = CIFAR10(root='./data', train=True, download=True, transform=test_transforms)\n",
    "\n",
    "# Step 3: Suddividere il dataset in Train (80%) e Validation (20%)\n",
    "train_size = int(0.8 * len(dataset_augmented))  # 80% dei dati\n",
    "val_size = len(dataset_augmented) - train_size  # 20% dei dati\n",
    "train_dataset, val_dataset = random_split(dataset_augmented, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
    "# take validation indices for not augmented dataset\n",
    "val_dataset = Subset(dataset_normal, val_dataset.indices)\n",
    "# Load CIFAR-10 dataset for testing\n",
    "test_dataset = CIFAR10(\n",
    "    root='./data', train=False, transform=test_transforms, download=True\n",
    ")\n",
    "# DataLoader for the training set\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "# DataLoader for the validation set\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=128, shuffle=False, num_workers=2\n",
    ")\n",
    "# DataLoader for the test set\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, num_workers=2\n",
    ")\n",
    "# Output the dataset sizes\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Val set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Augmented and Default Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize an image\n",
    "def denormalize(img):\n",
    "    # Reverse the normalization using mean and std\n",
    "    img = img * torch.tensor(std).view(3, 1, 1) + torch.tensor(mean).view(3, 1, 1)\n",
    "    # Clamp values to be in the range [0, 1] for valid image visualization\n",
    "    return img.clamp(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dataset\n",
    "dataiter = iter(train_loader)  # Get a batch of training data\n",
    "images, labels = next(dataiter)  # Extract images and labels from the batch\n",
    "\n",
    "# Create a grid of 1x5 subplots for visualization\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "\n",
    "for i in range(5):\n",
    "    img = denormalize(images[i])  # Denormalize the image\n",
    "    img = np.transpose(img.numpy(), (1, 2, 0))  # Convert from CHW format to HWC format for plotting\n",
    "    axes[i].imshow(img)  # Display the image\n",
    "    axes[i].axis('off')  # Remove axis for cleaner visualization\n",
    "\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Dataset\n",
    "dataiter = iter(val_loader)  # Get a batch of training data\n",
    "images, labels = next(dataiter)  # Extract images and labels from the batch\n",
    "\n",
    "# Create a grid of 1x5 subplots for visualization\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "\n",
    "for i in range(5):\n",
    "    img = denormalize(images[i])  # Denormalize the image\n",
    "    img = np.transpose(img.numpy(), (1, 2, 0))  # Convert from CHW format to HWC format for plotting\n",
    "    axes[i].imshow(img)  # Display the image\n",
    "    axes[i].axis('off')  # Remove axis for cleaner visualization\n",
    "\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet + Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout to be set\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=drop_rate)  # first dropout added\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # dropout after Relu\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Layers\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0])\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Dropout(p=drop_rate),  # second dropout before fc\n",
    "        nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, out_channels, stride))\n",
    "        self.in_planes = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call and visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models from scratch\n",
    "drop_rate = 0.1\n",
    "resnet18_from_scratch = ResNet18(num_classes=10) \n",
    "# set the chosen model\n",
    "model = resnet18_from_scratch.to(device)  \n",
    "# show the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification tasks\n",
    "optimizer = optim.Adam(model.parameters())  # Adam optimizer for model parameters\n",
    "\n",
    "# Training and validation functions\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move data to device (GPU/CPU)\n",
    "        optimizer.zero_grad()  # Zero the gradients from the previous step\n",
    "        outputs = model(inputs)  # Forward pass through the model\n",
    "        loss = criterion(outputs, targets)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "        _, predicted = outputs.max(1)  # Get the predicted class\n",
    "        total += targets.size(0)  # Total number of samples\n",
    "        correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "            outputs = model(inputs)  # Forward pass through the model\n",
    "            loss = criterion(outputs, targets)  # Compute the loss\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "            _, predicted = outputs.max(1)  # Get the predicted class\n",
    "            total += targets.size(0)  # Total number of samples\n",
    "            correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50  # Set the number of epochs\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    # Validate the model\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    \n",
    "    # Record the loss and accuracy for each epoch\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Print the results for the current epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results on Test Set\n",
    "test_loss, test_acc = validate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet+ SVM end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace the cross entropy loss with the Square Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function with Square Hinge Loss + L2 Regularization\n",
    "def square_hinge_loss(outputs, targets, weights, C=10):\n",
    "    \n",
    "    # Create a one-hot tensor for the targets (true classes)\n",
    "    targets_one_hot = torch.full_like(outputs, -1, device=outputs.device)  # All classes initially set to -1\n",
    "    targets_one_hot[torch.arange(len(targets)), targets] = 1  # Set the correct class to +1\n",
    "    \n",
    "    # Calculate the margin\n",
    "    margins = 1 - targets_one_hot * outputs  # shape: (n_samples, n_classes)\n",
    "    # Square hinge loss: max(0, margin)^2\n",
    "    hinge_loss = torch.clamp(margins, min=0) ** 2  # shape: (n_samples, n_classes)\n",
    "    \n",
    "    # Average over all samples and classes\n",
    "    hinge_loss = hinge_loss.mean()\n",
    "\n",
    "    # Calculate L2 regularization (mean of squared weights)\n",
    "    reg_loss = torch.mean(torch.square(weights))\n",
    "    # Total loss with the regularization parameter C\n",
    "    total_loss = C * hinge_loss + reg_loss\n",
    "    \n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the Architechture adding a normalization of the embedding features extracted by the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetSVM(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Layers\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0])\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.layer_norm = nn.LayerNorm(512) #normalization added\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Dropout(p=drop_rate),  # second dropout before fc\n",
    "        nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, out_channels, stride))\n",
    "        self.in_planes = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def ResNet18SVM(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "drop_rate= 0.1\n",
    "C= 10\n",
    "# Set model\n",
    "resnet18_from_scratch = ResNet18SVM(num_classes=10)  # Initialize ResNet18 with 10 output classes\n",
    "model = resnet18_from_scratch.to(device)  # Move the model to the specified device (GPU/CPU)\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters())  # Adam optimizer for model parameters\n",
    "\n",
    "# Training function\n",
    "def train(model, loader, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move data to the device\n",
    "        optimizer.zero_grad()  # Zero the gradients from the previous step\n",
    "        \n",
    "        # Get model outputs (logits)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get weights of the final layer\n",
    "        readout_weights = model.fc[-1].weight\n",
    "\n",
    "\n",
    "        # Calculate the square hinge loss with L2 regularization\n",
    "        loss = square_hinge_loss(outputs, targets, readout_weights, C = C)\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        _, predicted = outputs.max(1)  # Get the predicted class\n",
    "        total += targets.size(0)  # Total number of samples\n",
    "        correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to the device\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get weights of the final layer\n",
    "            readout_weights = model.fc[-1].weight\n",
    "\n",
    "\n",
    "            # Calculate the square hinge loss with L2 regularization\n",
    "            loss = square_hinge_loss(outputs, targets, readout_weights, C = C)\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "            _, predicted = outputs.max(1)  # Get the predicted class\n",
    "            total += targets.size(0)  # Total number of samples\n",
    "            correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader,optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results on Test Set\n",
    "test_loss, test_acc = validate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-34 + Kernel SVM end-to-end using Random Fourier Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the class fot the RFF transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFourierFeatures(nn.Module):\n",
    "    def __init__(self, input_dim, D=None, gamma=1.0):\n",
    "        super(RandomFourierFeatures, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.D = D if D is not None else input_dim  # Numero di RFF (default uguale a input_dim)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Calcolare la deviazione standard in base a gamma\n",
    "        std_dev = np.sqrt(2 * self.gamma)\n",
    "        \n",
    "        # Campionamento dei pesi w da N(0, 2*gamma)\n",
    "        self.register_buffer(\"weights\", torch.normal(0, std_dev, size=(self.D, self.input_dim)))\n",
    "        \n",
    "        # Campionamento del bias da U[0, 2pi]\n",
    "        self.register_buffer(\"bias\", 2 * np.pi * torch.rand(self.D))\n",
    "        \n",
    "        # Precalcolo del fattore di scala\n",
    "        self.register_buffer(\"scale\", torch.sqrt(torch.tensor(2.0 / self.D)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Proiezione lineare e applicazione del coseno\n",
    "        projections = torch.matmul(x, self.weights.T) + self.bias\n",
    "        return self.scale * torch.cos(projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify the CNN architecture adding the kernel aproximation vector coming from RFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "drop_rate=0.1\n",
    "embedding_dim = 512\n",
    "rff_dim = 2048\n",
    "gamma = 0.01\n",
    "# Set model\n",
    "resnet18_from_scratch = ResNet18SVM(num_classes=10)  \n",
    "model = resnet18_from_scratch\n",
    "# new layer with RFF and no drop out\n",
    "model.fc = nn.Sequential(\n",
    "    RandomFourierFeatures(input_dim=embedding_dim, D=rff_dim, gamma=gamma),   \n",
    "    nn.Linear(rff_dim, 10)  \n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters())  # Adam optimizer for model parameters\n",
    "\n",
    "# Training function\n",
    "def train(model, loader, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move data to the device\n",
    "        optimizer.zero_grad()  # Zero the gradients from the previous step\n",
    "        \n",
    "        # Get model outputs (logits)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get weights of the final layer\n",
    "        readout_weights = model.fc[-1].weight\n",
    "\n",
    "\n",
    "        # Calculate the square hinge loss with L2 regularization\n",
    "        loss = square_hinge_loss(outputs, targets, readout_weights, C = 0.1)\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        _, predicted = outputs.max(1)  # Get the predicted class\n",
    "        total += targets.size(0)  # Total number of samples\n",
    "        correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to the device\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get weights of the final layer\n",
    "            readout_weights = model.fc[-1].weight\n",
    "\n",
    "\n",
    "            # Calculate the square hinge loss with L2 regularization\n",
    "            loss = square_hinge_loss(outputs, targets, readout_weights, C = 0.1)\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "            _, predicted = outputs.max(1)  # Get the predicted class\n",
    "            total += targets.size(0)  # Total number of samples\n",
    "            correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader,optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"svm_rff_020_01_005.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results on Test Set\n",
    "test_loss, test_acc = validate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
